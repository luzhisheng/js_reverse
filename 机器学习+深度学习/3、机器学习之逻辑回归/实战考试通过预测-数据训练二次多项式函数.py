from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import PolynomialFeatures
from æ”¯æŒä¸­æ–‡ import init_plot
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

SHOW_PLOT = init_plot()

# 1. è¯»å–æ•°æ®
data = pd.read_csv('../æ•°æ®é›†/examdata.csv')
X = data.drop(['Pass'], axis=1)
y = data['Pass']

# 2. äºŒé˜¶å¤šé¡¹å¼ç‰¹å¾
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# 3. åˆ›å»ºå¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
model = LogisticRegression(max_iter=5000)  # è¿­ä»£æ¬¡æ•°å¤šä¸€ç‚¹ï¼Œé˜²æ­¢ä¸æ”¶æ•›
model.fit(X_poly, y)

# 4. é¢„æµ‹ & å‡†ç¡®ç‡
y_predict = model.predict(X_poly)
accuracy = accuracy_score(y, y_predict)

# 5. ç»˜åˆ¶æ•£ç‚¹
passed = data[y == 1]
failed = data[y == 0]
plt.scatter(passed['Exam1'], passed['Exam2'], color='blue', label='é€šè¿‡')
plt.scatter(failed['Exam1'], failed['Exam2'], color='red', label='æœªé€šè¿‡')

# 6. ç”Ÿæˆç½‘æ ¼ç‚¹å¹¶è½¬æ¢ä¸ºå¤šé¡¹å¼ç‰¹å¾
x1_min, x1_max = X['Exam1'].min() - 5, X['Exam1'].max() + 5
x2_min, x2_max = X['Exam2'].min() - 5, X['Exam2'].max() + 5
xx1, xx2 = np.meshgrid(
    np.linspace(x1_min, x1_max, 300),
    np.linspace(x2_min, x2_max, 300)
)
grid_points = np.c_[xx1.ravel(), xx2.ravel()]
grid_points_df = pd.DataFrame(grid_points, columns=['Exam1', 'Exam2'])
grid_points_poly = poly.transform(grid_points_df)

# 7. é¢„æµ‹ç½‘æ ¼ç‚¹ç±»åˆ«ï¼ˆç”¨æ¦‚ç‡ç”» 0.5 è¾¹ç•Œï¼‰
probs = model.predict_proba(grid_points_poly)[:, 1]
probs = probs.reshape(xx1.shape)
plt.contour(xx1, xx2, probs, levels=[0.5], linewidths=2, colors='green')

# 8. å›¾å½¢è®¾ç½®
plt.xlabel("åˆ†æ•° 1")
plt.ylabel("åˆ†æ•° 2")
plt.title(f"äºŒé˜¶å¤šé¡¹å¼é€»è¾‘å›å½’ï¼ˆå‡†ç¡®ç‡: {accuracy:.2f}ï¼‰")
plt.legend()
plt.grid(True)
plt.axis('equal')

# 9. æ˜¾ç¤ºæˆ–ä¿å­˜
if SHOW_PLOT:
    plt.show()
else:
    plt.savefig("logistic_poly2.png", dpi=300, bbox_inches='tight')
    print("ğŸ“· å·²ä¿å­˜å›¾ç‰‡ logistic_poly2.png")
